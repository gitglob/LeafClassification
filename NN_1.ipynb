{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64EuvcuD9GF1"
   },
   "source": [
    "## Exercise 9\n",
    "\n",
    "**Tip** This is a very small dataset (number of observations) compared to the number of features.\n",
    "This means that overfitting may be an issue, and sometimes fancy tricks won't do any good. \n",
    "Keep that in mind, and always start simple.\n",
    "\n",
    "**3.1) Improve the network**, and get as high a validation score as you can. \n",
    "When trying to improve the network nothing is sacred. You can try various learning rates, batch sizes, validation sizes, etc. \n",
    "And most importantly, the validation set is very small (only 1 sample per class), etc.\n",
    "\n",
    "To get you off to a good start we have created a list of **things you might want to try**:\n",
    "* Add more layers (mostly fully connected and convolutional)\n",
    "* Increase or decrease the batch size \n",
    "* Use dropout (a lot - e.g. between the convolutional layers)\n",
    "* Use batch normalization (a lot)\n",
    "* Try with L2 regularization (weight decay)\n",
    "* Use only the image for training (with CNN) - comment on the increased time between iterations.\n",
    "* Change the image size to be bigger or smaller\n",
    "* Try other combinations of FFN, CNN, RNN parts in various ways (bigger is not always better)\n",
    "\n",
    "If your network is not performing as well as you would like it to, [here](http://theorangeduck.com/page/neural-network-not-working) is a great explanation of what might have gone wrong.\n",
    "\n",
    "\n",
    "**3.2) Improve Kaggle score**. Once happy try to get the best score on Kaggle for this dataset as you can (**upload** instructions below)\n",
    "You can upload your solution multiple times as you progress.\n",
    "A very good implementation would get a score between $0.04$ to $0.06$ (the smaller the better), try and see if you can get there, and explain what might have gone wrong if you can't. \n",
    "\n",
    "\n",
    "**3.3) Reflect on the process**, and how you got to your final design and discuss your final results. \n",
    "What worked, and what didn't?\n",
    "Include at least the following: \n",
    "* Description of the final architecture\n",
    "* Description of the training parameters\n",
    "* Description of the final results (Kaggle and validation)\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move to the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/zhome/68/a/154632/Documents/dl/LeafClassification\n"
     ]
    }
   ],
   "source": [
    "cd ~/Documents/dl/LeafClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_1_EXE_Kaggle_Leaf_Challenge.ipynb  \u001b[0m\u001b[38;5;27m__pycache__\u001b[0m/   sample_submission.csv\n",
      "NN.ipynb                             data_utils.py  test.csv\n",
      "NN_1.ipynb                           \u001b[38;5;27mimages\u001b[0m/        train.csv\n",
      "Preprocessing.ipynb                  my_process.md\n",
      "Submission.ipynb                     \u001b[38;5;27mpickles\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GyUQS_w3iwdy"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "AXSG__0hivvb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "import data_utils\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
    "from skimage import io\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if CUDA device is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPU.\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('pickles/data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7eaSVfsvKos"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "zLfyufVYjBDh"
   },
   "outputs": [],
   "source": [
    "class SelectItem(nn.Module):\n",
    "    def __init__(self, item_index):\n",
    "        super(SelectItem, self).__init__()\n",
    "        self._name = 'selectitem'\n",
    "        self.item_index = item_index\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs[self.item_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "hSg83cnaY_9R"
   },
   "outputs": [],
   "source": [
    "def conv_size(img_size, kernel_size, stride, padding, channels):\n",
    "    W = img_size\n",
    "    K = kernel_size\n",
    "    P = padding\n",
    "    S = stride\n",
    "\n",
    "    output_size = int((W-K+(2*P))/S)+1\n",
    "\n",
    "    return (output_size, output_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "NxxSsEcObVtp"
   },
   "outputs": [],
   "source": [
    "def pool_size(img_size, kernel_size, stride, channels):\n",
    "    I = img_size\n",
    "    F = kernel_size\n",
    "    S = stride\n",
    "\n",
    "    output_size = (((I - F) / S) + 1)\n",
    "    \n",
    "    return (output_size, output_size, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 8)\n",
      "(32.0, 32.0, 8)\n"
     ]
    }
   ],
   "source": [
    "print(conv_size(128, 3, 2, 1, 8))\n",
    "print(pool_size(64, 2, 2, 8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (128, 128, 1)\n",
    "NUM_CLASSES =  99\n",
    "batch_size = 32\n",
    "# For all three features types margin, shape, and texture, we have NUM_FEATURES for each type.\n",
    "NUM_FEATURES = 64  # <-- Your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_out_channels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 106\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# print(out['out'].shape) \u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[39m# print(out)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m--> 106\u001b[0m net \u001b[39m=\u001b[39m Net()\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m use_cuda:\n\u001b[1;32m    108\u001b[0m     net\u001b[39m.\u001b[39mcuda()\n",
      "Cell \u001b[0;32mIn[69], line 9\u001b[0m, in \u001b[0;36mNet.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      5\u001b[0m     \u001b[39msuper\u001b[39m(Net, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvolutional \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m      8\u001b[0m         nn\u001b[39m.\u001b[39mConv2d(in_channels\u001b[39m=\u001b[39mchannels,\n\u001b[0;32m----> 9\u001b[0m                 out_channels\u001b[39m=\u001b[39mconv_out_channels,\n\u001b[1;32m     10\u001b[0m                 kernel_size\u001b[39m=\u001b[39mkernel_size,\n\u001b[1;32m     11\u001b[0m                 stride\u001b[39m=\u001b[39mconv_stride,\n\u001b[1;32m     12\u001b[0m                 padding\u001b[39m=\u001b[39mconv_pad),\n\u001b[1;32m     13\u001b[0m         nn\u001b[39m.\u001b[39mReLU()\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     17\u001b[0m         nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m32768\u001b[39m,\n\u001b[1;32m     18\u001b[0m                 out_features\u001b[39m=\u001b[39m\u001b[39m768\u001b[39m,\n\u001b[1;32m     19\u001b[0m                 bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     22\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     23\u001b[0m         nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,\n\u001b[1;32m     24\u001b[0m                 out_features\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m,\n\u001b[1;32m     25\u001b[0m                 bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_out_channels' is not defined"
     ]
    }
   ],
   "source": [
    "rnn_input_size = 64 # must be the same as the x_shape channels\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.convolutional = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=channels,\n",
    "                    out_channels=conv_out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=conv_stride,\n",
    "                    padding=conv_pad),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=32768,\n",
    "                    out_features=768,\n",
    "                    bias=False)\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=128,\n",
    "                    out_features=128,\n",
    "                    bias=False)\n",
    "        )\n",
    "\n",
    "        # Exercise: Add a recurrent unit like and RNN or GRU\n",
    "        # >> YOUR CODE HERE <<\n",
    "        self.recurrent = nn.Sequential(\n",
    "            nn.GRU(input_size=rnn_input_size, # The number of expected features in the input x\n",
    "                    hidden_size=128, # The number of features in the hidden state h\n",
    "                    num_layers=2), # Number of recurrent layers\n",
    "            SelectItem(0)\n",
    "        )\n",
    "\n",
    "        self.l_out = nn.Sequential(\n",
    "            nn.Linear(in_features=features_cat_size,\n",
    "                        out_features=NUM_CLASSES,\n",
    "                        bias=False)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x_img, x_margin, x_shape, x_texture):\n",
    "        features = []\n",
    "        out = {}\n",
    "\n",
    "        # Change layer order in images\n",
    "        x_img = x_img.permute(0, 3, 1, 2)\n",
    "        \n",
    "        ## Convolutional layer ##\n",
    "        # - Change dimensions to fit the convolutional layer \n",
    "        # - Apply Conv2d\n",
    "        # - Use an activation function\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "        \n",
    "        # >> YOUR CODE HERE <<\n",
    "        ## 1st way, distort image\n",
    "        # print(\"Convolutional...\")\n",
    "        # print(x_img.shape)\n",
    "        x_img = self.convolutional(x_img)\n",
    "        # print(x_img.shape)\n",
    "        x_img = x_img.reshape(x_img.size(0), -1)\n",
    "        # print(x_img.shape)\n",
    "        features_img = self.fc1(x_img)\n",
    "        # print(features_img.shape)\n",
    "\n",
    "        # Append features to the list \"features\"\n",
    "        features.append(features_img)\n",
    "        \n",
    "        ## Use concatenated leaf features for FFNN ##\n",
    "        # print(\"\\nFeed Forward...\")\n",
    "        # print(x_margin.shape, x_texture.shape)\n",
    "        x = torch.cat((x_margin, x_texture), dim=1)  # if you want to use features as feature vectors\n",
    "        # print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        features_vector = x\n",
    "        # print(features_vector.shape)\n",
    "        \n",
    "        ## Use concatenated leaf features for RNN ##\n",
    "        # - Chage dimensions to fit GRU\n",
    "        # - Apply GRU\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "\n",
    "        # >> YOUR CODE HERE <<\n",
    "        # print(\"\\nRecurrent...\")\n",
    "        # print(x_shape.shape)\n",
    "        features_rnn = self.recurrent(x_shape)\n",
    "        features.append(features_rnn)\n",
    "        # print(features_rnn.shape)\n",
    "        \n",
    "        # Append features to the list \"features\"\n",
    "        features.append(features_rnn)\n",
    "        \n",
    "        ## Output layer where all features are in use ##\n",
    "        # print(\"\\Features...\")\n",
    "        features_final = torch.cat(features, dim=1)\n",
    "        # print(features_final.shape)\n",
    "        \n",
    "        # print(\"\\nOutput...\")\n",
    "        out['out'] = self.l_out(features_final)\n",
    "        # print(out['out'].shape) \n",
    "        # print(out)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6E37RQPRcRS",
    "outputId": "b965bb90-1181-4d3c-8b60-bc6286d1f2af"
   },
   "outputs": [],
   "source": [
    "_img_shape = tuple([batch_size] + list(IMAGE_SHAPE))\n",
    "_feature_shape = (batch_size, NUM_FEATURES)\n",
    "\n",
    "def randnorm(size):\n",
    "    return np.random.normal(0, 1, size).astype('float32')\n",
    "\n",
    "# dummy data\n",
    "_x_image = get_variable(Variable(torch.from_numpy(randnorm(_img_shape))))\n",
    "_x_margin = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "_x_shape = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "_x_texture = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "\n",
    "# test the forward pass\n",
    "print(f\"Image shape: {_x_image.shape}\")\n",
    "print(f\"Image margin: {_x_margin.shape}\")\n",
    "print(f\"Image texture: {_x_texture.shape}\")\n",
    "print(f\"Image shape: {_x_shape.shape}\\n\")\n",
    "output = net(x_img=_x_image, x_margin=_x_margin, x_shape=_x_shape, x_texture=_x_texture)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYPRBHH0HHi0"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "num_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# weight_decay is equal to L2 regularization\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # , momentum=0.9)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iI1CIeK4U6gO"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g9e15p2jRQq0",
    "outputId": "7b4fd27e-1d41-4743-d47b-a4b00015d909"
   },
   "outputs": [],
   "source": [
    "# Setup settings for training \n",
    "VALIDATION_SIZE = 0.3 # 0.1 is ~ 100 samples for validation\n",
    "max_iter = 1000\n",
    "log_every = 100\n",
    "eval_every = 100\n",
    "\n",
    "# Function to get label\n",
    "def get_labels(batch):\n",
    "    return get_variable(Variable(torch.from_numpy(batch['ts']).long()))\n",
    "\n",
    "# Function to get input\n",
    "def get_input(batch):\n",
    "    return {\n",
    "        'x_img': get_variable(Variable(torch.from_numpy(batch['images']))),\n",
    "        'x_margin': get_variable(Variable(torch.from_numpy(batch['margins']))),\n",
    "        'x_shape': get_variable(Variable(torch.from_numpy(batch['shapes']))),\n",
    "        'x_texture': get_variable(Variable(torch.from_numpy(batch['textures'])))\n",
    "    }\n",
    "\n",
    "# Initialize lists for training and validation\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "\n",
    "# Generate batches\n",
    "batch_gen = data_utils.batch_generator(data,\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_classes=NUM_CLASSES,\n",
    "                                       num_iterations=max_iter,\n",
    "                                       seed=42,\n",
    "                                       val_size=VALIDATION_SIZE)\n",
    "\n",
    "# Train network\n",
    "net.train()\n",
    "for i, batch_train in enumerate(batch_gen.gen_train()):\n",
    "    # print(_['x_img'].shape, _['x_margin'].shape, _['x_texture'].shape, _['x_shape'].shape)\n",
    "    if i % eval_every == 0:\n",
    "        \n",
    "        # Do the validaiton\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "        for batch_valid, num in batch_gen.gen_valid():\n",
    "            output = net(**get_input(batch_valid))\n",
    "            labels_argmax = torch.max(get_labels(batch_valid), 1)[1]\n",
    "            val_losses += criterion(output['out'], labels_argmax) * num\n",
    "            val_accs += accuracy(output['out'], labels_argmax) * num\n",
    "            val_lengths += num\n",
    "\n",
    "        # Divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        valid_loss.append(get_numpy(val_losses))\n",
    "        valid_accs.append(get_numpy(val_accs))\n",
    "        valid_iter.append(i)\n",
    "        print(\"Valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, valid_loss[-1], valid_accs[-1]))\n",
    "        net.train()\n",
    "    \n",
    "    # Train network\n",
    "    output = net(**get_input(batch_train))\n",
    "    labels_argmax = torch.max(get_labels(batch_train), 1)[1]\n",
    "    batch_loss = criterion(output['out'], labels_argmax)\n",
    "    \n",
    "    train_iter.append(i)\n",
    "    train_loss.append(float(get_numpy(batch_loss)))\n",
    "    train_accs.append(float(get_numpy(accuracy(output['out'], labels_argmax))))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log i figure\n",
    "    if i % log_every == 0:\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_iter, train_loss, label='train_loss')\n",
    "        plt.plot(valid_iter, valid_loss, label='valid_loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_iter, train_accs, label='train_accs')\n",
    "        plt.plot(valid_iter, valid_accs, label='valid_accs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        # clear_output(wait=True)\n",
    "        print(\"Train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, train_loss[-1], train_accs[-1]))\n",
    "        \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "kuNFmtv6vOSB"
   },
   "source": [
    "# Advanced Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1RPZYUXFvUIJ"
   },
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "-s6imHe0vTk8"
   },
   "outputs": [],
   "source": [
    "class LeafDataset(Dataset):\n",
    "    \"\"\"Leaf dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None, train=False, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.leafs_df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.leafs_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # parse the image\n",
    "        img_name = str(self.leafs_df.iloc[idx, 0]) + '.jpg'\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = io.imread(img_path)\n",
    "\n",
    "        # no matter what happens, we need to padd all the images to the same dimensions, so that we can resize them without distorting them\n",
    "        image = data_utils.pad2square(image)  # Make the image square\n",
    "        image = resize(image, output_shape=(128, 128), mode='reflect', anti_aliasing=True)  # resizes the image\n",
    "\n",
    "        # augment the image if chosen to\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # if we have the trainset, then we have a label\n",
    "        if self.train:\n",
    "            # parse the rest of the data\n",
    "            id = self.leafs_df.iloc[idx, 0]\n",
    "            species = self.leafs_df.iloc[idx, 1]\n",
    "            margins = self.leafs_df.iloc[idx, 2:66]\n",
    "            shapes = self.leafs_df.iloc[idx, 66:130]\n",
    "            textures = self.leafs_df.iloc[idx, 130:]\n",
    "\n",
    "            # convert to tuple and return\n",
    "            sample = {'image': image, 'id': id, 'species': species, 'margins': margins, 'shapes': shapes, 'textures': textures}\n",
    "            X = {'image': image, \n",
    "                'margins': margins, \n",
    "                'shapes': shapes, \n",
    "                'textures': textures}\n",
    "            y = species\n",
    "            return X, y\n",
    "\n",
    "        if self.test:\n",
    "            # parse the rest of the data\n",
    "            id = self.leafs_df.iloc[idx, 0]\n",
    "            margins = self.leafs_df.iloc[idx, 1:65]\n",
    "            shapes = self.leafs_df.iloc[idx, 65:129]\n",
    "            textures = self.leafs_df.iloc[idx, 129:]\n",
    "\n",
    "            # convert to tuple and return\n",
    "            sample = {'image': image, \n",
    "                    'id': id, \n",
    "                    'margins': margins, \n",
    "                    'shapes': shapes, \n",
    "                    'textures': textures}\n",
    "            X = {'image': image, 'margins': margins, 'shapes': shapes, 'textures': textures}\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leafs_frame = pd.read_csv('train.csv')\n",
    "\n",
    "n = 65\n",
    "row = leafs_frame.iloc[n, :]\n",
    "id = leafs_frame.iloc[n, 0]\n",
    "img_name = str(id) + '.jpg'\n",
    "species = leafs_frame.iloc[n, 1]\n",
    "margins = leafs_frame.iloc[n, 2:66]\n",
    "shapes = leafs_frame.iloc[n, 66:130]\n",
    "textures = leafs_frame.iloc[n, 130:]\n",
    "\n",
    "print(f'Row shape: {row.shape}')\n",
    "print(f'ID: {id}')\n",
    "print(f'Species: {species}')\n",
    "print(f'Margins\\' shape: {margins.shape}')\n",
    "print(f'Shapes\\' shape: {shapes.shape}')\n",
    "print(f'Textures\\' shape: {textures.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trainset and testset using the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_csv = 'train.csv'\n",
    "test_csv = 'test.csv'\n",
    "root_dir = 'images/'\n",
    "\n",
    "batch_size = 32\n",
    "trainset = LeafDataset(train_csv, root_dir, transform=train_transform, train=False, test=False)\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testset = LeafDataset(test_csv, root_dir, transform=train_transform, train=False, test=False)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_leafs(species, image, margins, shapes, textures, id=None):\n",
    "    \"\"\"Show image and margin\"\"\"\n",
    "    fig = plt.figure(figsize=(20,3))\n",
    "    ax1 = fig.add_subplot(141)\n",
    "    ax1.imshow(image)\n",
    "    ax2 = fig.add_subplot(142)\n",
    "    ax2.plot(margins)\n",
    "    ax3 = fig.add_subplot(143)\n",
    "    ax3.plot(shapes)\n",
    "    ax4 = fig.add_subplot(144)\n",
    "    ax4.plot(textures)\n",
    "    plt.suptitle(f\"id: {id}, species: {species}\")\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    plt.show()\n",
    "\n",
    "# img_path = os.path.join('images/', img_name)\n",
    "# img = io.imread(img_path)\n",
    "# show_leafs(species, img, margins, textures, shapes, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_dataset = LeafDataset(csv_file='train.csv',\n",
    "                            root_dir='images/',\n",
    "                            train=True)\n",
    "\n",
    "for i in range(len(leaf_dataset)):\n",
    "    X, y = leaf_dataset[i]\n",
    "    print(i, len(X), X['image'].shape, X['margins'].shape, X['shapes'].shape, X['textures'].shape)\n",
    "\n",
    "    print(f'sample #{i}')\n",
    "    show_leafs(y, **X)\n",
    "\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oN7l36WoR6Oh"
   },
   "outputs": [],
   "source": [
    "# The image shape should be of the format (height, width, channels)\n",
    "IMAGE_SHAPE = (128, 128, 1)   # <-- Your answer here\n",
    "NUM_CLASSES =  99  # <-- Your answer here \n",
    "\n",
    "# For all three features types margin, shape, and texture, we have NUM_FEATURES for each type.\n",
    "NUM_FEATURES = 64  # <-- Your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQxXcMc5RMI1",
    "outputId": "d57b0e03-2601-4044-c1c3-14934b86b239"
   },
   "outputs": [],
   "source": [
    "height, width, channels = IMAGE_SHAPE\n",
    "batch_size = 32\n",
    "\n",
    "# Keep track of features to output layer\n",
    "features_cat_size = 1024 # <-- Number of features concatenated before output layer\n",
    "\n",
    "rnn_input_size = 64 # must be the same as the x_shape channels\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.convolutional = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 1,\n",
    "                    out_channels = 8,\n",
    "                    kernel_size = (3, 3),\n",
    "                    stride = 2,\n",
    "                    padding = 1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, \n",
    "                         stride = 2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(in_features=32*32*8,\n",
    "                    out_features=768,\n",
    "                    bias=False),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(in_features=128,\n",
    "                    out_features=128,\n",
    "                    bias=False),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        # Exercise: Add a recurrent unit like and RNN or GRU\n",
    "        # >> YOUR CODE HERE <<\n",
    "        self.recurrent = nn.Sequential(\n",
    "            nn.GRU(input_size=rnn_input_size, # The number of expected features in the input x\n",
    "                    hidden_size=128, # The number of features in the hidden state h\n",
    "                    num_layers=2), # Number of recurrent layers\n",
    "            SelectItem(0)\n",
    "        )\n",
    "\n",
    "        self.l_out = nn.Sequential(\n",
    "            nn.Linear(in_features=features_cat_size,\n",
    "                        out_features=NUM_CLASSES,\n",
    "                        bias=False)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x_img, x_margin, x_shape, x_texture):\n",
    "        features = []\n",
    "        out = {}\n",
    "\n",
    "        # Change layer order in images\n",
    "        x_img = x_img.permute(0, 3, 1, 2)\n",
    "        \n",
    "        ## Convolutional layer ##\n",
    "        # - Change dimensions to fit the convolutional layer \n",
    "        # - Apply Conv2d\n",
    "        # - Use an activation function\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "        \n",
    "        # >> YOUR CODE HERE <<\n",
    "        ## 1st way, distort image\n",
    "        # print(\"Convolutional...\")\n",
    "        # print(x_img.shape)\n",
    "        x_img = self.convolutional(x_img)\n",
    "        # print(x_img.shape)\n",
    "        x_img = x_img.reshape(x_img.size(0), -1)\n",
    "        # print(x_img.shape)\n",
    "        features_img = self.fc1(x_img)\n",
    "        # print(features_img.shape)\n",
    "\n",
    "        # Append features to the list \"features\"\n",
    "        features.append(features_img)\n",
    "        \n",
    "        ## Use concatenated leaf features for FFNN ##\n",
    "        # print(\"\\nFeed Forward...\")\n",
    "        # print(x_margin.shape, x_texture.shape)\n",
    "        x = torch.cat((x_margin, x_texture), dim=1)  # if you want to use features as feature vectors\n",
    "        # print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        features_vector = x\n",
    "        # print(features_vector.shape)\n",
    "        \n",
    "        ## Use concatenated leaf features for RNN ##\n",
    "        # - Chage dimensions to fit GRU\n",
    "        # - Apply GRU\n",
    "        # - Change dimensions s.t. the features can be used in the final FFNN output layer\n",
    "\n",
    "        # >> YOUR CODE HERE <<\n",
    "        # print(\"\\nRecurrent...\")\n",
    "        # print(x_shape.shape)\n",
    "        features_rnn = self.recurrent(x_shape)\n",
    "        features.append(features_rnn)\n",
    "        # print(features_rnn.shape)\n",
    "        \n",
    "        # Append features to the list \"features\"\n",
    "        features.append(features_rnn)\n",
    "        \n",
    "        ## Output layer where all features are in use ##\n",
    "        # print(\"\\Features...\")\n",
    "        features_final = torch.cat(features, dim=1)\n",
    "        # print(features_final.shape)\n",
    "        \n",
    "        # print(\"\\nOutput...\")\n",
    "        out['out'] = self.l_out(features_final)\n",
    "        # print(out['out'].shape) \n",
    "        # print(out)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img_shape = tuple([batch_size] + list(IMAGE_SHAPE))\n",
    "_feature_shape = (batch_size, NUM_FEATURES)\n",
    "\n",
    "def randnorm(size):\n",
    "    return np.random.normal(0, 1, size).astype('float32')\n",
    "\n",
    "# dummy data\n",
    "_x_image = get_variable(Variable(torch.from_numpy(randnorm(_img_shape))))\n",
    "_x_margin = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "_x_shape = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "_x_texture = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "\n",
    "# test the forward pass\n",
    "print(f\"Image shape: {_x_image.shape}\")\n",
    "print(f\"Image margin: {_x_margin.shape}\")\n",
    "print(f\"Image texture: {_x_texture.shape}\")\n",
    "print(f\"Image shape: {_x_shape.shape}\\n\")\n",
    "output = net(x_img=_x_image, x_margin=_x_margin, x_shape=_x_shape, x_texture=_x_texture)\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, opt, loss_fn, epochs, train_loader, test_loader):\n",
    "    epoch_results = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        clear_output(wait=True)\n",
    "        print('* Epoch %d/%d' % (epoch+1, epochs))\n",
    "\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        model.train()  # train mode\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data\n",
    "\n",
    "        # for X_batch, Y_batch in train_loader:\n",
    "        #     X_batch = X_batch.to(device)\n",
    "        #     Y_batch = Y_batch.to(device)\n",
    "\n",
    "            # set parameter gradients to zero\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)['out']\n",
    "            labels_argmax = torch.max(get_labels(inputs), 1)[1]\n",
    "            # calculate loss function\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # back-propagation\n",
    "            loss.backward()\n",
    "            # weight update\n",
    "            opt.step()\n",
    "\n",
    "            # calculate metrics to show the user\n",
    "            train_correct += (outputs == y_pred).sum().cpu().item()\n",
    "            train_loss += loss / len(train_loader)\n",
    "            \n",
    "        #print(' - accuracy: %f' % train_accuracy)\n",
    "        train_accuracy = train_correct/(len(trainset)*128*128)\n",
    "        print(' - train accuracy: %f' % train_accuracy)\n",
    "        print(' - train loss: %f' % train_loss)\n",
    "\n",
    "        # show intermediate results\n",
    "        # Compute the val accuracy\n",
    "        model.eval()  # testing mode\n",
    "        val_correct = 0\n",
    "        val_loss = 0\n",
    "        i = 0\n",
    "        for x_val, y_val in test_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)            \n",
    "            with torch.no_grad():\n",
    "                output, y_sigmoid = model(x_val)\n",
    "                y_pred = torch.where(y_sigmoid > 0.5, 1., 0.)\n",
    "                \n",
    "            if i==0:\n",
    "                x_ = x_val\n",
    "                output_ = output\n",
    "\n",
    "            loss = loss_fn(y_val, y_sigmoid)\n",
    "            val_loss += loss/len(test_loader)\n",
    "            val_correct += (y_val==y_pred).sum().cpu().item()\n",
    "            i+=1\n",
    "\n",
    "        print(' - val loss: %f' % val_loss)\n",
    "        val_accuracy = val_correct/(len(testset)*128*128)\n",
    "        print(' - val accuracy: %f' % val_accuracy)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        y_ = torch.sigmoid(output_).detach().cpu()\n",
    "        for k in range(6):\n",
    "            plt.subplot(2, 6, k+1)\n",
    "            plt.imshow(np.rollaxis(x_[k].cpu().numpy(), 0, 3), cmap='gray')\n",
    "            plt.title('Real')\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.subplot(2, 6, k+7)\n",
    "            plt.imshow(y_[k, 0], cmap='gray')\n",
    "            plt.title('Output')\n",
    "            plt.axis('off')\n",
    "        plt.suptitle('%d / %d - loss: %f - acc %f' % (epoch+1, epochs, train_loss, train_accuracy))\n",
    "        plt.show()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        epoch_results.append([train_accuracy, train_loss.item(), val_accuracy, val_loss.item()])\n",
    "    \n",
    "    return epoch_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "num_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# weight_decay is equal to L2 regularization\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # , momentum=0.9)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_11 = train(net, optimizer, criterion, num_epochs, train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
